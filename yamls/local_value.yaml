name: local-llama-8b-reward
image: mosaicml/dle:nightly-latest

scheduling:
  priority: high
  max_retries: 0
  preemptible: false
  retry_on_system_failure: false
compute:
  gpus: 8
  cluster: r5z2p1

parameters:
  seed: 17
  # loggers:
  # TODO: define loggers, e.g. mlflow, wandb, etc.
  # mlflow:
  #   tracking_uri: databricks
  #   experiment_name: debug_rm
  #   tags:
  #     run: "{run_name}"

  model:
    # name: hf_classifier_rm # for finegrained rewards
    #name: hf_pairwise_rm
    name: hf_classifier_vm
    pretrained_model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct #Qwen/ #meta-llama/Meta-Llama-3.1-8B-Instruct
    pretrained: true
    loss_type: ce
    use_flash_attention_2: true
    return_last: false
    return_lm_logits: false
    use_auth_token: true
    init_device: mixed
    #n_labels: 2
    #additional_train_metrics:
    #- pairwise_rm_accuracy
    #additional_eval_metrics:
    #- pairwise_rm_accuracy
    # For regression based rewards
    # additional_train_metrics:
    # - classifier_accuracy
    # additional_eval_metrics:
    # - classifier_accuracy


  callbacks:
    lr_monitor: {}
    speed_monitor:
      window_size: 10
    memory_monitor: {}

  optimizer:
    lr: 5.0e-06
    eps: 1.0e-10
    name: decoupled_adamw
    betas:
    - 0.9
    - 0.95
    weight_decay: 0.0

  precision: amp_bf16

  scheduler:
    name: cosine_with_warmup
    alpha_f: 0.01
    t_warmup: 0.1dur

  tokenizer:
    name: Qwen/Qwen2.5-1.5B-Instruct #meta-llama/Meta-Llama-3.1-8B-Instruct
    kwargs:
      model_max_length: ${max_seq_len}
      trust_remote_code: true

  algorithms:
    gradient_clipping:
      clipping_type: norm
      clipping_threshold: 1

  autoresume: false
  eval_first: false

  fsdp_config:
    sharding_strategy: FULL_SHARD
    cpu_offload: false
    mixed_precision: PURE
    activation_checkpointing: true
    activation_checkpointing_reentrant: false
    state_dict_type: sharded
    use_orig_params: true
    forward_prefetch: true
    limit_all_gathers: true

  max_seq_len: 4096
  save_folder: /tmp/reward_model    # TODO: insert proper save_folder
  dist_timeout: 1200
  max_duration: 1ep
  progress_bar: false

  # eval_loader:
  # - name: pairwise_preference
  #   label: val
  #   dataset:
  #     remote: # Add val path here if applicable
  #     split: val
  #     shuffle: false
  #     max_seq_len: ${max_seq_len}
  #     shuffle_seed: 17
  #   drop_last: true
  #   num_workers: 8

  train_loader:
    # name: finegrained_preference # For finegrained rewards
    name: pairwise_preference
    dataset:
      remote: dbfs:/Volumes/datasets/wensun/data/apo_math/Qwen2.5-7B/value_data/train/
      # remote: # TODO: Add remote path here if applicable
      # local: # TODO: Add the local path of the training ataloader
      shuffle: true
      max_seq_len: ${max_seq_len}
      shuffle_seed: 17
    drop_last: true
    num_workers: 8

  eval_interval: 1     # TODO: change if an eval loader is specified
  save_interval: 1ep
  log_to_console: true
  console_log_interval: 1ba
  device_eval_batch_size: 1
  eval_subset_num_batches: -1
  global_train_batch_size: 8     # TODO: update for more realistic batch sizes
  device_train_microbatch_size: 1

integrations:
- integration_type: git_repo
  path: custom-llm-foundry
  git_repo: mosaicml/llm-foundry
  git_branch: main
  pip_install: . --no-deps
- integration_type: git_repo
  git_repo: databricks/Compose-RL
  git_branch: wensun/value_modeling
  pip_install: .[gpu] --no-deps
env_variables:
  AWS_PROFILE: data-force-one

command: |-
  # Run llm foundry train

  cd llm-foundry/scripts/train/
  composer train.py /mnt/config/parameters.yaml
