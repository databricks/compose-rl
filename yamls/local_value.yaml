name: local-value-model-training
image: mosaicml/dle:nightly-latest

run_name: value_model_training

scheduling:
  priority: high
  max_retries: 0
  preemptible: true
  retry_on_system_failure: false
compute:
  gpus: 8
  cluster: r5z2p1

parameters:
  seed: 17
  
  loggers:
  # TODO: define loggers, e.g. mlflow, wandb, etc.
    mlflow:
      tracking_uri: databricks
      experiment_name: debug_value_training
      tags:
        run: "{run_name}"

  model:
    # name: hf_classifier_rm # for finegrained rewards
    #name: hf_pairwise_rm
    name: hf_classifier_vm
    pretrained_model_name_or_path: Qwen/Qwen2.5-7B-Instruct #Qwen/ #meta-llama/Meta-Llama-3.1-8B-Instruct
    pretrained: true
    loss_type: ce
    use_flash_attention_2: true
    return_last: false
    return_lm_logits: false
    use_auth_token: true
    #init_device: mixed
    #n_labels: 2
    #additional_train_metrics:
    #- pairwise_rm_accuracy
    #additional_eval_metrics:
    #- pairwise_rm_accuracy
    # For regression based rewards
    # additional_train_metrics:
    # - classifier_accuracy
    # additional_eval_metrics:
    # - classifier_accuracy


  callbacks:
    lr_monitor: {}
    speed_monitor:
      window_size: 10
    memory_monitor: {}

  optimizer:
    lr: 1.0e-06
    eps: 1.0e-10
    name: decoupled_adamw
    betas:
    - 0.9
    - 0.95
    weight_decay: 1.0e-08

  precision: amp_bf16

  scheduler:
    name: cosine_with_warmup
    alpha_f: 0.01
    t_warmup: 0.1dur

  tokenizer:
    name: Qwen/Qwen2.5-7B-Instruct #meta-llama/Meta-Llama-3.1-8B-Instruct
    kwargs:
      model_max_length: ${max_seq_len}
      trust_remote_code: true

  algorithms:
    gradient_clipping:
      clipping_type: norm
      clipping_threshold: 1

  autoresume: false
  eval_first: false

  fsdp_config:
    verbose: false
    cpu_offload: false
    mixed_precision: PURE
    state_dict_type: sharded
    use_orig_params: true
    forward_prefetch: true
    backward_prefetch: BACKWARD_PRE
    sharding_strategy: FULL_SHARD
    activation_checkpointing: true
    activation_cpu_offload: false
    activation_checkpointing_reentrant: false


  max_seq_len: 16384 #4096
  save_folder: /tmp/reward_model    # TODO: insert proper save_folder
  dist_timeout: 1200
  max_duration: 1ep
  progress_bar: false

  # eval_loader:
  # - name: pairwise_preference
  #   label: val
  #   dataset:
  #     remote: # Add val path here if applicable
  #     split: val
  #     shuffle: false
  #     max_seq_len: ${max_seq_len}
  #     shuffle_seed: 17
  #   drop_last: true
  #   num_workers: 8

  train_loader:
    # name: finegrained_preference # For finegrained rewards
    name: finegrained_preference #pairwise_preference
    dataset:
      remote: dbfs:/Volumes/datasets/wensun/data/apo_math/Qwen2.5-7B/value_data_new/train/
      # remote: # TODO: Add remote path here if applicable
      # local: # TODO: Add the local path of the training ataloader
      shuffle: true
      max_seq_len: ${max_seq_len}
      shuffle_seed: 17
    drop_last: true
    num_workers: 8

  eval_interval: 1     # TODO: change if an eval loader is specified
  save_interval: 1ep
  log_to_console: true
  console_log_interval: 1ba
  device_eval_batch_size: 1
  eval_subset_num_batches: -1
  global_train_batch_size: 16     # TODO: update for more realistic batch sizes
  device_train_microbatch_size: 2

integrations:
- integration_type: git_repo
  path: custom-llm-foundry
  git_repo: mosaicml/llm-foundry
  git_branch: main
  pip_install: . --no-deps
- integration_type: git_repo
  git_repo: databricks/Compose-RL
  git_branch: wensun/value_modeling
  pip_install: .[gpu] --no-deps
env_variables:
  AWS_PROFILE: data-force-one

command: |-
  # Run llm foundry train

  cd llm-foundry/scripts/train/
  composer train.py /mnt/config/parameters.yaml
