seed: 17
model:
  name: hf_critic_free_lm
  loss_type: grpo
  target_kl: 0.1
  pretrained: true
  init_device: mixed
  kl_estimator: k3
  kl_clip_range: 40
  use_auth_token: true
  compute_kl_loss: false
  policy_clip_ratio: 0.2
  attn_implementation: flash_attention_2
  normalize_advantage: true
  use_flash_attention_2: true
  length_normalize_policy_loss: true
  pretrained_model_name_or_path: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  allow_embedding_resizing: true
loggers:
  mlflow:
    tags:
      run:
        run_name: null
      group: grpo
    tracking_uri: databricks
    experiment_name: /Users/ashutosh.baheti@databricks.com/mlflow_experiments/online_rl/benchmark_math/llama_8b_grpo_lr_gc_sweep1
callbacks:
  ppo: {}
  orl_eval:
    evals:
    - name: gsm8k
    eval_overrides:
      generation_params:
        max_tokens: 8192
  lr_monitor: {}
  scheduled_gc:
    batch_interval: 1000
  speed_monitor:
    window_size: 1
  memory_monitor: {}
  hf_checkpointer:
    overwrite: true
    precision: bfloat16
    save_folder: /tmp/hf_checkpoints/
    save_interval: 1dur
  runtime_estimator: {}
optimizer:
  lr: 1.0e-06
  name: decoupled_adamw
  betas:
  - 0.9
  - 0.95
  weight_decay: 0
precision: amp_bf16
scheduler:
  name: constant_with_warmup
  alpha: 1
  t_warmup: 10iter
tokenizer:
  name: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  kwargs:
    padding: longest
    pad_token: "<|finetune_right_pad_id|>"
    truncation: true
    padding_side: left
    model_max_length: 6000
    trust_remote_code: true
variables:
  gamma: 1
  lambda_gae: 1
  global_seed: 17
  max_gen_len: 4000
  epoch_per_iteration: 1
  num_batches_per_update: 8
  generations_per_prompt: 8
  device_generate_batch_size: 1
  vllm_enable_prefix_caching: true
  generation_kwargs:
    top_p: 1.0
    use_cache: true
    do_sample: false
    temperature: 1.0
  eos_token_ids:
    - 128001
    - 128008
    - 128009
  buffer:
    name: MinibatchRolloutBuffer
    max_buffer_size: 8
  kl_controller:
    init_kl_coef: 0.0
    kl_ctl_type: fixed
  tokenizer_name: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
  num_train_nodes: 1
  reference_model:
    precision: amp_bf16
    pretrained: true
    model_config:
      name: hf_causal_lm
      pretrained: true
      use_auth_token: true
      use_flash_attention_2: true
      pretrained_model_name_or_path: deepseek-ai/DeepSeek-R1-Distill-Llama-8B
    load_path: null
  non_train_fsdp_config:
    verbose: false
    cpu_offload: false
    mixed_precision: PURE
    state_dict_type: sharded
    use_orig_params: true
    forward_prefetch: true
    backward_prefetch: BACKWARD_PRE
    sharding_strategy: FULL_SHARD
    activation_cpu_offload: false
    activation_checkpointing: true
    activation_checkpointing_reentrant: false
  vllm_tensor_parallel_size: 1
  rewards:
    math_verifier:
      reward: 4
      reward_type: math_verifier
    bad_generation_end:
      reward: -1
      eos_penalty: true
      reward_type: bad_generation_end
    math_format_verifier:
      reward: 1
      reward_type: math_format_verifier
    penalize_extra_short_responses:
      reward: -1
      reward_type: short_response_reward
      len_threshold: 10
algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1.0
autoresume: true
log_config: true
fsdp_config:
  verbose: false
  cpu_offload: false
  mixed_precision: PURE
  state_dict_type: sharded
  use_orig_params: true
  forward_prefetch: true
  backward_prefetch: BACKWARD_PRE
  sharding_strategy: FULL_SHARD
  activation_cpu_offload: false
  activation_checkpointing: true
  activation_checkpointing_reentrant: false
max_seq_len: 6000
save_folder: /tmp/checkpoints
dist_timeout: 1800
max_duration: 50iter
progress_bar: false
train_loader:
  name: prompt
  dataset:
    local: /tmp/dataset/prompt_{timestamp}/
    split: train
    remote: dbfs:/Volumes/datasets/ashutoshbaheti/orl_data/open_r1_filtered/dpsk_8b_open_r1_48k/
    shuffle: true
    max_gen_len: 4000
    max_seq_len: 6000
    shuffle_seed: 17
    download_timeout: 1800
  drop_last: true
  num_workers: 1
eval_interval: 5iter
save_interval: 100iter
log_to_console: true
save_overwrite: true
python_log_level: debug
console_log_interval: 1ba
device_eval_batch_size: 1
eval_subset_num_batches: -1
global_train_batch_size: 64
device_train_microbatch_size: 2
save_num_checkpoints_to_keep: 1