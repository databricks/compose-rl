run_name: local-llama-8b-reward
seed: 17
# loggers:
# TODO: define loggers, e.g. mlflow, wandb, etc.
# mlflow:
#   tracking_uri: databricks
#   experiment_name: debug_rm
#   tags:
#     run: "{run_name}"

model:
  # name: hf_classifier_rm # for finegrained rewards
  name: hf_pairwise_rm
  pretrained_model_name_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct
  pretrained: true
  loss_type: bt
  use_flash_attention_2: true
  return_last: true
  return_lm_logits: false
  use_auth_token: true
  init_device: mixed
  additional_train_metrics:
  - pairwise_rm_accuracy
  additional_eval_metrics:
  - pairwise_rm_accuracy
  # For regression based rewards
  # additional_train_metrics:
  # - classifier_accuracy
  # additional_eval_metrics:
  # - classifier_accuracy


callbacks:
  lr_monitor: {}
  speed_monitor:
    window_size: 10
  memory_monitor: {}

optimizer:
  lr: 5.0e-06
  eps: 1.0e-10
  name: decoupled_adamw
  betas:
  - 0.9
  - 0.95
  weight_decay: 0.0

precision: amp_bf16

scheduler:
  name: cosine_with_warmup
  alpha_f: 0.01
  t_warmup: 0.1dur

tokenizer:
  name: meta-llama/Meta-Llama-3.1-8B-Instruct
  kwargs:
    model_max_length: ${max_seq_len}
    trust_remote_code: true

algorithms:
  gradient_clipping:
    clipping_type: norm
    clipping_threshold: 1

autoresume: false
eval_first: false

fsdp_config:
  sharding_strategy: FULL_SHARD
  cpu_offload: false
  mixed_precision: PURE
  activation_checkpointing: true
  activation_checkpointing_reentrant: false
  state_dict_type: sharded
  use_orig_params: true
  forward_prefetch: true
  limit_all_gathers: true

max_seq_len: 4096
save_folder: /tmp/reward_model    # TODO: insert proper save_folder
dist_timeout: 1200
max_duration: 1ep
progress_bar: false

# eval_loader:
# - name: pairwise_preference
#   label: val
#   dataset:
#     remote: # Add val path here if applicable
#     split: val
#     shuffle: false
#     max_seq_len: ${max_seq_len}
#     shuffle_seed: 17
#   drop_last: true
#   num_workers: 8

train_loader:
  # name: finegrained_preference # For finegrained rewards
  name: pairwise_preference
  dataset:
    # remote: # TODO: Add remote path here if applicable
    # local: # TODO: Add the local path of the training ataloader
    shuffle: true
    max_seq_len: ${max_seq_len}
    shuffle_seed: 17
  drop_last: true
  num_workers: 8

eval_interval: 1     # TODO: change if an eval loader is specified
save_interval: 1ep
log_to_console: true
console_log_interval: 1ba
device_eval_batch_size: 1
eval_subset_num_batches: -1
global_train_batch_size: 8     # TODO: update for more realistic batch sizes
device_train_microbatch_size: 1
